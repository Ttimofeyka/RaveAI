/*
This Source Code Form is subject to the terms of the Mozilla
Public License, v. 2.0. If a copy of the MPL was not distributed
with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
*/

import <std/math> <std/random> <std/io>
import "utils"

namespace rai {
    namespace LayerType {
        alias Linear = 0;
        alias Activation = 1;
        alias Dropout = 2;
    }

    namespace ActivationType {
        alias ReLU = 0;
        alias Sigmoid = 1;
        alias Tanh = 2;
    }

    struct LinearLayer {
        float* input;
        float* weights;
        float* output;
        float* grad;

        int inSize;
        int wSize;
        int outSize;

        rai::LinearLayer this(int inSize, int outSize) {
            rai::LinearLayer this;

            this.inSize = inSize;
            this.outSize = outSize;
            this.wSize = inSize * outSize;

            this.output = std::new<float>(outSize);
            this.weights = std::new<float>(this.wSize);
            this.grad = std::new<float>(outSize);
        } => this;

        void randomize(int min, int max) {
            for(int i=0; i<wSize; i++) weights[i] = std::randomInt(min, max) / 1000f;
        }

        void forward(float* input) {
            this.input = input;

            for(int i=0; i<outSize; i++) {
                output[i] = 0f;

                for(int j=0; j<inSize; j++) output[i] += input[j] * weights[(i * inSize) + j];
            }
        }

        void backward(float* predicted, float lRate) {
            for(int i=0; i<outSize; i++) {
                for(int j=0; j<inSize; j++) {
                    weights[(i * inSize) + j] -= lRate * predicted[i] * input[j];
                }
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.weights);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.weights = null;
            this.grad = null;
        }
    }

    struct ActivationLayer {
        int size;
        float* input;
        float* output;
        float* grad;
        int type;

        rai::ActivationLayer this(int size, int type) {
            rai::ActivationLayer this;

            this.size = size;
            this.type = type;

            this.output = std::new<float>(size);
            this.grad = std::new<float>(size);
        } => this;

        void forward(float* input) {
            this.input = input;

            for(int i=0; i<size; i++) {
                switch(type) {
                    case(rai::ActivationType::ReLU) output[i] = std::math::relu(input[i]);
                    case(rai::ActivationType::Sigmoid) output[i] = std::math::sigmoid(input[i]);
                    case(rai::ActivationType::Tanh) output[i] = std::math::tanh(input[i]);
                    default output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(int i=0; i<size; i++) {
                switch(type) {
                    case(rai::ActivationType::ReLU) grad[i] = predicted[i] * std::math::drelu(output[i]);
                    case(rai::ActivationType::Sigmoid) grad[i] = predicted[i] * std::math::dsigmoid(output[i]);
                    case(rai::ActivationType::Tanh) {
                        float value = std::math::tanh(output[i]);
                        grad[i] = predicted[i] * (1f - (value * value));
                    }
                    default grad[i] = predicted[i] * output[i];
                }
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.grad = null;
        }
    }

    struct DropoutLayer {
        int size;
        float prob; // 0-1
        float* output;

        rai::DropoutLayer this(int size, float prob) {
            rai::DropoutLayer this;

            this.size = size;
            this.prob = prob;
            this.output = std::new<float>(size);
        } => this;

        void forward(float* input, bool isTraining) {
            for(int i=0; i<size; i++) {
                if(isTraining) {
                    if((std::randomInt(0, 100) / 100f) <= prob) output[i] = 0f;
                    else output[i] = input[i];
                }
                else {
                    for(int i=0; i<size; i++) output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(int i=0; i<size; i++) output[i] *= predicted[i];
        }

        void ~this {
            std::free(this.output);

            this.output = null;
        }
    }
}