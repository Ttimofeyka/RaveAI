/*
This Source Code Form is subject to the terms of the Mozilla
Public License, v. 2.0. If a copy of the MPL was not distributed
with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
*/

import <std/math> <std/random> <std/io> <std/blas>
import "utils" "optimizers"

namespace rai {
    namespace LayerType {
        alias Linear = 0;
        alias Activation = 1;
        alias Dropout = 2;
        alias MinGRU = 3;
    }

    namespace ActivationType {
        alias ReLU = 0;
        alias Sigmoid = 1;
        alias Tanh = 2;
    }

    struct LinearLayer {
        float* input;
        float* weights;
        float* output;
        float* grad;

        int inSize;
        int wSize;
        int outSize;

        rai::LinearLayer this(int inSize, int outSize) {
            rai::LinearLayer this;

            this.inSize = inSize;
            this.outSize = outSize;
            this.wSize = inSize * outSize;

            this.output = std::anew<float>(16, outSize);
            this.weights = std::anew<float>(16, this.wSize);
            this.grad = std::anew<float>(16, outSize);
        } => this;

        void randomize(int min, int max) {
            for(int i=0; i<wSize; i++) weights[i] = std::randomInt(min, max) / 1000f;
        }

        void forward(float* input) {
            this.input = input;

            std::blas::vecmatmul<float>(inSize, outSize, input, weights, output);
        }

        void backward(float* predicted, float lRate) {
            for(int i=0; i<outSize; i++) {
                std::blas::axpy<float>(inSize, -lRate * predicted[i], input, &weights[i * inSize]);
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.weights);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.weights = null;
            this.grad = null;
        }
    }

    struct MinGRULayer {
        int inSize, outSize;

        float* wZ, dWZ; // in -> hidden
        float* wO, dWO; // hidden -> out
        float* h; // Hidden

        float* hPrev; // Previous states
        float* z; // Gate
        float* hTilde; // Candidates

        // TODO: is seq_len needed?        

        rai::MinGRULayer this(int inSize, int outSize) {
            rai::MinGRULayer this;

            this.inSize = inSize;
            this.outSize = outSize;

            this.wZ = std::anew<float>(16, inSize);
            this.wO = std::anew<float>(16, outSize);
            
            this.dWZ = std::anew<float>(16, inSize);
            this.dWO = std::anew<float>(16, outSize);

            this.h = std::anew<float>(16, outSize);
            this.z = std::anew<float>(16, inSize * outSize);
            this.hTilde = std::anew<float>(16, inSize * outSize);

            this.hPrev = null;

            for(int i=0; i<inSize; i++) {
                this.wZ[i] = 0f;
                this.dWZ[i] = 0f;
            }

            for(int i=0; i<outSize; i++) {
                this.wO[i] = 0f;
                this.dWO[i] = 0f;
                this.h[i] = 0f;
            }

            for(int i=0; i < inSize * outSize; i++) {
                this.z[i] = 0f;
                this.hTilde[i] = 0f;
            }
        } => this;

        void parameters(std::vector<rai::OptPair>* buffer) {
            buffer.add(rai::OptPair{inSize, wZ, dWZ});
            buffer.add(rai::OptPair{outSize, wO, dWO});
        }

        std::vector<rai::OptPair> parameters {return = std::vector<rai::OptPair>(); parameters(&return);}

        void randomize(int min, int max) {
            for(int i=0; i<inSize; i++) wZ[i] = (std::randomInt(min, max) / 1000f);
            for(int i=0; i<outSize; i++) wO[i] = (std::randomInt(min, max) / 1000f);
        }

        void forward(float* input) {
            // TODO
        }

        void backward(float* predicted) {
            // TODO
        }

        void ~this {
            
        }
    }

    struct ActivationLayer {
        int size;
        float* input;
        float* output;
        float* grad;
        int type;

        rai::ActivationLayer this(int size, int type) {
            rai::ActivationLayer this;

            this.size = size;
            this.type = type;

            this.output = std::anew<float>(16, size);
            this.grad = std::anew<float>(16, size);
        } => this;

        void forward(float* input) {
            this.input = input;

            for(int i=0; i<size; i++) {
                switch(type) {
                    case(rai::ActivationType::ReLU) output[i] = std::math::relu(input[i]);
                    case(rai::ActivationType::Sigmoid) output[i] = std::math::sigmoid(input[i]);
                    case(rai::ActivationType::Tanh) output[i] = std::math::tanh(input[i]);
                    default output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(int i=0; i<size; i++) {
                switch(type) {
                    case(rai::ActivationType::ReLU) grad[i] = predicted[i] * std::math::drelu(output[i]);
                    case(rai::ActivationType::Sigmoid) grad[i] = predicted[i] * std::math::dsigmoid(output[i]);
                    case(rai::ActivationType::Tanh) {
                        float value = std::math::tanh(output[i]);
                        grad[i] = predicted[i] * (1f - (value * value));
                    }
                    default grad[i] = predicted[i] * output[i];
                }
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.grad = null;
        }
    }

    struct DropoutLayer {
        int size;
        float prob; // 0-1
        float* output;

        rai::DropoutLayer this(int size, float prob) {
            rai::DropoutLayer this;

            this.size = size;
            this.prob = prob;
            this.output = std::anew<float>(16, size);
        } => this;

        void forward(float* input, bool isTraining) {
            for(int i=0; i<size; i++) {
                if(isTraining) {
                    if((std::randomInt(0, 100) / 100f) <= prob) output[i] = 0f;
                    else output[i] = input[i];
                }
                else {
                    for(int i=0; i<size; i++) output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(int i=0; i<size; i++) output[i] *= predicted[i];
        }

        void ~this {
            std::free(this.output);

            this.output = null;
        }
    }
}