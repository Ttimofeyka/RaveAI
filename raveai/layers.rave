/*
This Source Code Form is subject to the terms of the Mozilla
Public License, v. 2.0. If a copy of the MPL was not distributed
with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
*/

import <std/math> <std/random> <std/io> <std/blas>
import "utils" "optimizers"

extern (linkname: "printf", vararg) void printf(char* fmt);

namespace rai {
    namespace LayerType {
        alias Linear = 0;
        alias Activation = 1;
        alias Dropout = 2;
        alias MinGRU = 3;
    }

    namespace ActivationType {
        alias ReLU = 0;
        alias Sigmoid = 1;
        alias Tanh = 2;
    }

    struct LinearLayer {
        float* input;
        float* weights;
        float* output;
        float* grad;

        int inSize;
        int wSize;
        int outSize;

        rai::LinearLayer this(int inSize, int outSize) {
            rai::LinearLayer this;

            this.inSize = inSize;
            this.outSize = outSize;
            this.wSize = inSize * outSize;

            this.output = std::anew<float>(16, outSize);
            this.weights = std::anew<float>(16, this.wSize);
            this.grad = std::anew<float>(16, outSize);
        } => this;

        void randomize(int min, int max) {
            for(int i=0; i<wSize; i++) weights[i] = std::randomInt(min, max) / 1000f;
        }

        void forward(float* input) {
            this.input = input;

            std::blas::vecmatmul<float>(inSize, outSize, input, weights, output);
        }

        void backward(float* predicted, float lRate) {
            for(int i=0; i<outSize; i++) {
                std::blas::axpy<float>(inSize, -lRate * predicted[i], input, &weights[i * inSize]);
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.weights);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.weights = null;
            this.grad = null;
        }
    }

    struct MinGRULayer {
        int inSize, outSize, wSize;

        float* wZ, dWZ; // in * hidden
        float* wO, dWO; // hidden * in
        float* h; // Hidden output
        float* h_prev; // Previous hidden output
        float* z, htilde; // Intermediate buffers
        float* grad; // Grad

        rai::MinGRULayer this(int inSize, int outSize) {
            rai::MinGRULayer this;

            this.inSize = inSize;
            this.outSize = outSize;
            this.wSize = inSize * outSize;

            this.wZ = std::anew<float>(16, inSize * this.wSize);
            this.wO = std::anew<float>(16, outSize * this.wSize);
            
            this.dWZ = std::anew<float>(16, inSize * this.wSize);
            this.dWO = std::anew<float>(16, outSize * this.wSize);

            this.h = std::anew<float>(16, outSize);
            this.z = std::anew<float>(16, outSize);
            this.h_prev = std::anew<float>(16, outSize);
            this.htilde = std::anew<float>(16, outSize);
            this.grad = std::anew<float>(16, outSize);

            for(int i=0; i < this.wSize; i++) {
                this.wZ[i] = 0f;
                this.wO[i] = 0f;
                this.dWZ[i] = 0f;
                this.dWO[i] = 0f;
            }

            for(int i=0; i < outSize; i++) {
                this.h[i] = 0f;
                this.z[i] = 0f;
                this.h_prev[i] = 0f;
                this.htilde[i] = 0f;
                this.grad[i] = 0f;
            }
        } => this;

        void parameters(std::vector<rai::OptPair>* buffer) {
            buffer.add(rai::OptPair{wSize, wZ, dWZ});
            buffer.add(rai::OptPair{wSize, wO, dWO});
        }

        std::vector<rai::OptPair> parameters {return = std::vector<rai::OptPair>(); parameters(&return);}

        void randomize(int min, int max) {
            for(int i=0; i < wSize; i++) {
                wZ[i] = (std::randomInt(min, max) / 1000f);
                wO[i] = (std::randomInt(min, max) / 1000f);
            }
        }

        void forward(float* input) {
            for(int j=0; j<outSize; j++) {
                /* Compute z[j] = sigmoid( wZ[j] · x ) */
                float az = 0f;
                for(int i = 0; i < inSize; i++) az += wZ[j * inSize + i] * input[i];
                z[j] = std::math::sigmoid(az);

                /* Compute htilde[j] = wO[j] · x */
                float ah = 0f;
                for(int i = 0; i < inSize; i++) ah += wO[j * inSize + i] * input[i];
                htilde[j] = ah;

                /* Hidden update: h = (1 - z) * h_prev + z * htilde */
                h[j] = (1.0 - z[j]) * h_prev[j] + z[j] * htilde[j];
            }
        }

        void backward(float* predicted) {
            for(int j=0; j<wSize; j++) {
                dWZ[j] = 0f;
                dWO[j] = 0f;
            }

            for(int j=0; j<outSize; j++) {
                float dh_j = grad[j];

                /* Gradients wrt htilde and z */
                float d_htilde = dh_j * z[j];
                float d_z = dh_j * (htilde[j] - h_prev[j]);

                /* Gradient through derivative sigmoid */
                float dz_da = std::math::dsigmoid(z[j]);
                float d_az = d_z * dz_da;

                /* Accumulate gradients into weight matrices */
                for(int i=0; i<inSize; i++) {
                    dWO[j * inSize + i] += d_htilde * predicted[i];
                    dWZ[j * inSize + i] += d_az * predicted[i];
                }
            }
        }

        void ~this {
            if(this.wZ == null) @return();
            std::free(this.wZ);
            std::free(this.wO);
            std::free(this.dWZ);
            std::free(this.dWO);
            std::free(this.h);
            std::free(this.z);
            std::free(this.h_prev);
            std::free(this.htilde);
            std::free(this.grad);
            this.wZ = null;
        }
    }

    struct ActivationLayer {
        uint size;
        float* input;
        float* output;
        float* grad;
        int type;

        rai::ActivationLayer this(uint size, int type) {
            rai::ActivationLayer this;

            this.size = size;
            this.type = type;

            this.output = std::anew<float>(16, size);
            this.grad = std::anew<float>(16, size);
        } => this;

        void forward(float* input) {
            this.input = input;

            for(uint i=0; i<size; i++) {
                switch(type) { // TODO: optimize & benchmark
                    case(rai::ActivationType::ReLU) output[i] = std::math::relu(input[i]);
                    case(rai::ActivationType::Sigmoid) output[i] = std::math::sigmoid(input[i]);
                    case(rai::ActivationType::Tanh) output[i] = std::math::tanh(input[i]);
                    default output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(uint i=0; i<size; i++) {
                switch(type) { // TODO: optimize & benchmark
                    case(rai::ActivationType::ReLU) grad[i] = predicted[i] * std::math::drelu(output[i]);
                    case(rai::ActivationType::Sigmoid) grad[i] = predicted[i] * std::math::dsigmoid(output[i]);
                    case(rai::ActivationType::Tanh) {
                        float value = std::math::tanh(output[i]);
                        grad[i] = predicted[i] * (1f - (value * value));
                    }
                    default grad[i] = predicted[i] * output[i];
                }
            }
        }

        void ~this {
            std::free(this.output);
            std::free(this.grad);

            this.input = null;
            this.output = null;
            this.grad = null;
        }
    }

    struct DropoutLayer {
        int size;
        float prob; // 0-1
        float* output;

        rai::DropoutLayer this(int size, float prob) {
            rai::DropoutLayer this;

            this.size = size;
            this.prob = prob;
            this.output = std::anew<float>(16, size);
        } => this;

        void forward(float* input, bool isTraining) {
            for(int i=0; i<size; i++) {
                if(isTraining) {
                    if((std::randomInt(0, 100) / 100f) <= prob) output[i] = input[i] * 0.1f;
                    else output[i] = input[i];
                }
                else {
                    for(int i=0; i<size; i++) output[i] = input[i];
                }
            }
        }

        void backward(float* predicted) {
            for(int i=0; i<size; i++) output[i] *= predicted[i];
        }

        void ~this {
            std::free(this.output);

            this.output = null;
        }
    }
}