import <std/io> <std/vector>
import "../../raveai/layers" "../../raveai/optimizers" "logits"

// MUST BE RUNNED FROM THE MAIN DIRECTORY!

uint contextSize = 16;

void main {
    std::file sh = std::file("shakespeare.txt");
    sh.open("r");
        std::string dataset = sh.readAll();
    sh.close();

    std::vector<std::vector<float*>> input = std::vector<std::vector<float*>>();
    std::vector<float*> target = std::vector<float*>();

    for(uint i=0; i < (dataset.length - contextSize); i++) {
        std::vector<float*> sequence = std::vector<float*>();
        fdefer ~sequence;

        for(uint j=0; j<contextSize; j++) {
            float* newLogits = std::anew<float>(16, vocabSize);
            fdefer std::free(newLogits);

            charToLogits(newLogits, dataset.data[i + j]);
            sequence.add(newLogits);
        }

        input.add(sequence);
        
        float* newLogits = std::anew<float>(16, vocabSize);
        fdefer std::free(newLogits);

        charToLogits(newLogits, dataset.data[i + contextSize]);
        target.add(newLogits);

        if(i % 10000 == 0) std::println("Tokenization step: ", i, " / ", dataset.length - contextSize);
    }

    rai::MinGRULayer layer = rai::MinGRULayer(vocabSize, vocabSize);
    layer.randomize(0, 1000);
    defer ~layer;

    // Good start for finding SGD lRate is 0.00001
    // rai::SGD optimizer = rai::SGD(layer.parameters(), 0.00001);
    // defer ~optimizer;

    // Good start for finding RMSProp lRate is 0.001
    rai::RMSProp optimizer = rai::RMSProp(layer.parameters(), 0.01, 0.9f, 0.000000001f);
    defer ~optimizer;

    // Training
    for(int epoch=0; epoch<10; epoch++) {
        float total = 0f;

        for(int sample=0; sample<input.length; sample++) {
            for(int i=0; i<layer.outSize; i++) layer.h_prev[i] = 0f;

            std::vector<float*> currSample = input.data[sample];

            // Forward
            for(uint i=0; i<currSample.length; i++) layer.forward(currSample.data[i]);

            // Backward
            total += rai::mseLossGrad(layer.h, target.data[sample], layer.grad, layer.outSize);

            layer.backward(target.data[sample]);

            optimizer.update();

            if(sample % 500 == 0) std::println("Step ", sample, " / ", input.length);
        }

        std::println("Epoch ", epoch, ", loss: ", total);
    }

    std::println("Done!");
}
